# Toxic_Spans_Detection

### Task description

The Toxic Spans Detection task concerns the evaluation of systems that detect the spans that make a text toxic, when detecting such spans is possible. Moderation is crucial to promoting healthy online discussions. Although several toxicity (a.k.a. abusive language) detection datasets (Wulczyn et al., 2017; Borkan et al., 2019) and models (Schmidt and Wiegand, 2017; Pavlopoulos et al., 2017b; Zampieri et al., 2019) have been released, most of them classify whole comments or documents, and do not identify the spans that make a text toxic. But highlighting such toxic spans can assist human moderators (e.g., news portals moderators) who often deal with lengthy comments, and who prefer attribution instead of just a system-generated unexplained toxicity score per post. The evaluation of systems that could accurately locate toxic spans within a text is thus a crucial step towards successful semi-automated moderation. \
\
For more information, visit the following links \
Task website : https://sites.google.com/view/toxicspans \
Codalab page : https://competitions.codalab.org/competitions/25623 \
Github repo : https://github.com/ipavlopoulos/toxic_spans

Toxic span detection is Task 5 in [SemEval-2021](https://semeval.github.io/SemEval2021/)
